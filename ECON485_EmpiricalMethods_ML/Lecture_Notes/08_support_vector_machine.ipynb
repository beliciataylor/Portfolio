{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit88832310db0f46c6ba1b97f6fce48e8e",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "What is a Hyperplane?\n",
    "* in a k dimensional space, a Hyperplane is a flat affine subspace of dimension k-1\n",
    "* in two dimensions, a Hyperplane is a flat one-dimensional subspaces i.e a line\n",
    "* in 3-dimensions, a hyperplane is a flat two-dimensional subspace i.e a plane\n",
    "* k > 3 dimensions, hard to visualize\n",
    "* notion of a (k-1)-dimensional flat subspace still applies\n",
    "\n",
    "math def of a hyperplane:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0$$\n",
    "\n",
    "* when saying the above equation \"defines\" the hyperplane, we're saying that any $X = (X_1,X_2)'$ for which the equation holds is a point on the hyperplane\n",
    "\n",
    "the multidimensional case one has\n",
    "\n",
    "$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k = 0$\n",
    "\n",
    "On the other hand, if \n",
    "\n",
    "$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k < 0$\n",
    "\n",
    "then X lies on the other side of the hyperplane\n",
    "\n",
    "* think of the hyperplane has dividing k-dimensional space into two halves\n",
    "* one can easily determine on which side of the hyperplane a point lies by calculating the sign of the lefthand side of the above equation\n",
    "\n",
    "![](img\\hyperplane_1.png)\n",
    "\n",
    "* hyperplane $1 + 2X_1 + 3X_2 = 0$ is shown above. Blue region is the set of points for which $1 + 2X_1 + 3X_2 > 0$, and the purple region is the set of points for which $1 + 2X_1 + 3X_2 < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using a Separating Hyperparameter\n",
    "\n",
    "![](img\\hyperplane_2.png)\n",
    "\n",
    "* suppose we have observations $\\{(y_1, x_{1,1}, x_{1,2}), (y_2, x_{2,1}, x_{2,2}), \\dots, (y_n, x_{n,1}, x_{n,2})$\n",
    "* we know these n obs fall into two classes: $\\{y_1, y_2, \\dots, y_n \\} \\in \\{-1,1\\}$, where $-1$ represents one class and $1$ represents the other class\n",
    "* the left graph is showing three out of many possible separating hyperplanes\n",
    "* the right graph is showing the decision rule made by a classifier based on this particular hyperplane (black line); if a test observation falls in the blue portion of the grid, it will be assigned to the case, and vise versa\n",
    "* a separating hyperplane for any k has the property that $\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_k x_{i,k}~>0~\\text{if}~y_1 = 1$ and $\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_k x_{i,k}~<0~\\text{if}~y_1 = -1$\n",
    "* equivalently, a separating hyperplane has the property that $y_i(\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_k x_{i,k}) > 0$ for all $i = 1,\\dots,n$ since $y_i \\in \\{-1,1\\}$\n",
    "\n",
    "EX. Imagine we're given a test observation $\\textbf{x}^* = [x_1^*, \\dots, x_k^*]'$, then we 'assign' it to a class based on the sign of $\\textbf{x}^{*'} \\beta = f(\\textbf{x}^*)$\n",
    "* if $f(\\textbf{x}^*) > 0$, then we assign this test observation to class 1, and if $f(\\textbf{x}^*)<0$, then we assign it to class -1\n",
    "\n",
    "The magnitude of $f(\\textbf{x}^*)$ is also useful\n",
    "* $f(\\textbf{x}^*)$ being far from zero makes us confident about its classification\n",
    "* when $f(\\textbf{x}^*)$ is close to zero, then $\\textbf{x}^*$ is located near the hyperplane, therefore we're less confident about the class assignment for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Maximal Margin Classifier\n",
    "\n",
    "![](img\\hyperplane_3.png)\n",
    "\n",
    "* can compute the perpendicular distance from each training observation to a given separating hyperplane\n",
    "* the smallest such distance is the minimal distance from the observations to the hyperplane, also known as the $\\textit{margin}$\n",
    "* the maximal margin hyperplane is the separating hyperplane for which the margin is largest; it's the hyperplane that has the farthest minimum distance to the training observations\n",
    "* we can then classify a test obseration based on which side of the maximal margin hyperplane it lies, which is known as $\\textit{maximal margin classifier}$\n",
    "\n",
    "EX. we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin\n",
    "* these three observations are known as $\\textit{support vectors}$ since they are vectors in k-dimensional space (here, k=2)\n",
    "* they support the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplane would move as well\n",
    "\n",
    "following chunks of code\n",
    "* simulated data set of 10,000 observations\n",
    "* objective is to build a machine that can predict loan default (no or yes) based on the balance and income of the customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/beliciarodriguez/Documents/GitHub/ECON485-Material-Review/data/hyperplane_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  default student      balance        income\n0      No      No   729.526495  44361.625074\n1      No     Yes   817.180407  12106.134700\n2      No      No  1073.549164  31767.138947\n3      No      No   529.250605  35704.493935\n4      No      No   785.655883  38463.495879",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>default</th>\n      <th>student</th>\n      <th>balance</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>No</td>\n      <td>No</td>\n      <td>729.526495</td>\n      <td>44361.625074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>No</td>\n      <td>Yes</td>\n      <td>817.180407</td>\n      <td>12106.134700</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>No</td>\n      <td>1073.549164</td>\n      <td>31767.138947</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>No</td>\n      <td>529.250605</td>\n      <td>35704.493935</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>No</td>\n      <td>No</td>\n      <td>785.655883</td>\n      <td>38463.495879</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "default     object\nstudent     object\nbalance    float64\nincome     float64\ndtype: object"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   default  student      balance        income\n0        0        0   729.526495  44361.625074\n1        0        1   817.180407  12106.134700\n2        0        0  1073.549164  31767.138947\n3        0        0   529.250605  35704.493935\n4        0        0   785.655883  38463.495879",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>default</th>\n      <th>student</th>\n      <th>balance</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>729.526495</td>\n      <td>44361.625074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>817.180407</td>\n      <td>12106.134700</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1073.549164</td>\n      <td>31767.138947</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>529.250605</td>\n      <td>35704.493935</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>785.655883</td>\n      <td>38463.495879</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# map the 'no' and 'yes' strings into numerical values for numerical calculations\n",
    "df['default'] = df['default'].map({'Yes': 1, 'No': 0})\n",
    "df['student'] = df['student'].map({'Yes': 1, 'No': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "default      int64\nstudent      int64\nbalance    float64\nincome     float64\ndtype: object"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the separating hyperplane might not exist, and so there may be nomaximal margin classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "y, X = patsy.dmatrices('default ~ -1 + balance + income', data=df, return_type='dataframe')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "* the SVC classifies a test observation depending on which side of the hyperplane it lies\n",
    "* the hyperplane is chosen to correctly separate most of the training observations into two classes, but may misclassify a few observations\n",
    "* it is the solution to the optimization problem\n",
    "\n",
    "\\begin{align*}\n",
    "    \\underset{\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}, \\epsilon_{1}, \\ldots, \\epsilon_{n}}{\\operatorname{maximize}} M\n",
    "    {\\text { subject to } \\sum_{j=1}^{k} \\beta_{j}^{2}=1} \\\\\n",
    "    {y_{i}\\left(\\beta_{0}+\\beta_{1} x_{i,1}+\\beta_{2} x_{i,2}+\\ldots+\\beta_{k} x_{i,k}\\right) \\geq M\\left(1-\\epsilon_{i}\\right)} \\\\\n",
    "    {\\quad \\epsilon_{i} \\geq 0, \\quad \\sum_{i=1}^{n} \\epsilon_{i} \\leq C}\n",
    "\\end{align*}\n",
    "\n",
    "* (cont) where $C$ is a nonnegative tuning parameter, $M$ is the width of the margin (which we want to make as large as possible)\n",
    "* $\\epsilon_1, \\dots, \\epsilon_n$ are slack variables that allow observations to be on the wrong side of the margin\n",
    "\n",
    "(1) the slack variable $\\epsilon_i$ tells us where the ith observation is located relative to the hyperplane and the margin i.e\n",
    "\n",
    "1. $\\epsilon_i = 0$: the ith observation is on the correct side of the margin\n",
    "2. $\\epsilon_i > 0$: the ith observation is on the wrong side of the margin\n",
    "3. $\\epsilon_i > 1$: the ith observation is on the wrong side of the hyperplane\n",
    "\n",
    "(2) the tuning parameter C bounds the sum of the $\\epsilon_i$'s and ca be considered a tolerance parameter\n",
    "\n",
    "1. If $C=0$ then we are not allowing for violations; must be the case that $\\epsilon_1 = \\dots = \\epsilon_n = 0$, in which case we have the maximal margin classifier (if it exists)\n",
    "2. For $C > 0$, no more than $C$ observations can be on the wrong side of the hyperplane\n",
    "* in this case $\\epsilon_i > 1$\n",
    "* we have that $\\sum_{i=1}^n \\epsilon_i \\leq C$\n",
    "* as $C$ increases, we become more tolerant of violations to the margin; the margin will widen\n",
    "* as $C$ decreases, we become less tolerant of violations to the margin; the margin narrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9676\n"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build your classifier\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "\n",
    "# Train it on the entire training data set\n",
    "clf.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Assessing the fit\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* now choose the hyperparameter $C$ by 3-fold CV over a grid of potential values for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=None, error_score=nan,\n             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n                           class_weight=None, coef0=0.0,\n                           decision_function_shape='ovr', degree=3,\n                           gamma='scale', kernel='linear', max_iter=-1,\n                           probability=False, random_state=None, shrinking=True,\n                           tol=0.001, verbose=False),\n             iid='deprecated', n_jobs=-1,\n             param_grid={'C': array([1.00000000e-06, 3.59381366e-06, 1.29154967e-05, 4.64158883e-05,\n       1.66810054e-04, 5.99484250e-04, 2.15443469e-03, 7.74263683e-03,\n       2.78255940e-02, 1.00000000e-01])},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=0)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),n_jobs=-1)\n",
    "clf.fit(X_train_scaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9663999999999999"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1e-06"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "clf.best_estimator_.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9676"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Prediction performance on test set is better that on train set\n",
    "clf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "* SVC is a natural approach for classification in the two class setting if the boundary between the two classes is linear\n",
    "* in practice, however, we are sometimes faced with non-linear class boundaries\n",
    "\n",
    "![](img\\hyperplane_4.png)\n",
    "\n",
    "* left graph: the observations fall into two classes with a non-linear boundary between them\n",
    "* right graph: the support vector classifier seeks a linear boundary, and consequently performs very poorly\n",
    "* turns out that the solution to the SVC problem inolves only the inner products between the point $\\textbf{x}$ and the support vectors\n",
    "* if $\\mathcal{S}$ is the collection of indices of these support points, we can rewrite any solution function as $f(\\textbf{x}) = \\beta_0 + \\sum_{i \\in \\mathcal{S}} \\alpha_i (\\textbf{x,x_i})$ where $<a,b> = \\sum_{j=1}^r a_j b_j$\n",
    "* therefore we can generalize this solution to $f(\\textbf{x}) = \\beta_0 + \\sum_{i \\in \\mathcal{S}} \\alpha_i K(\\textbf{x,x_i})$ where $K$ is the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9724\n0.1\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9712"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "svcPoly = SVC(kernel='poly',degree=3)\n",
    "\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svcPoly, param_grid=dict(C=Cs),n_jobs=-1)\n",
    "clf.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "print(clf.best_score_ )\n",
    "print(clf.best_estimator_.C  )\n",
    "\n",
    "# Prediction performance on test set is not better that on train set\n",
    "clf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9663999999999999\n1e-06\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9676"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "svcRadial = SVC(kernel='rbf',gamma=.01)\n",
    "\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svcRadial, param_grid=dict(C=Cs),n_jobs=-1)\n",
    "clf.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "print(clf.best_score_ )\n",
    "print(clf.best_estimator_.C  )\n",
    "\n",
    "# Prediction performance on test set is not better that on train set\n",
    "clf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}