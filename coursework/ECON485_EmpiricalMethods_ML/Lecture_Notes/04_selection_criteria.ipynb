{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Criteria: MLR\n",
    "\n",
    "* goal is to list select criteria for the linear regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "\n",
    "# import dataset hprice3\n",
    "hprice3 = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/hprice3.dta')\n",
    "\n",
    "# log lcbd variable\n",
    "hprice3['lcbd'] = np.log(hprice3.cbd)\n",
    "\n",
    "# set up two linear regression model where model1 includes linstsq and agesq\n",
    "f1 = 'lprice ~ lland + larea + lcbd + nbh + rooms + y81 + linst + linstsq + ldist + baths + age + agesq'\n",
    "f2 = 'lprice ~ lland + larea + lcbd + nbh + rooms + y81 + linst           + ldist + baths + age        '\n",
    "y1, X1 = patsy.dmatrices(f1, data=hprice3, return_type='dataframe')\n",
    "y2, X2 = patsy.dmatrices(f2, data=hprice3, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the following equation criterias:\n",
    "\n",
    "Adjusted $\\bar{R}^2$\n",
    "\n",
    "$\\bar{R}^2 = 1 - (1-R^2)\\frac{n-1}{n-K-1}$ where $R^2$ := standard regression coefficient of determination\n",
    "\n",
    "Bayesian Information Criterion\n",
    "\n",
    "$BIC = n + nlog(2\\pi \\hat{\\sigma}^2) + Klog(n)$\n",
    "\n",
    "Akaike Information Criterion\n",
    "\n",
    "$AIC = n + nlog(2\\pi\\hat{\\sigma}^2) + 2K$\n",
    "\n",
    "* now we will do what is often done in stat learning literature and use BIC and AIC defined without additive constants $n + nlog(2\\pi)$:\n",
    "\n",
    "$IC = nlog(\\hat{\\sigma}^2) + c(n,K)$\n",
    "\n",
    "* one has AIC when $c = 2K$ and BIC when $c = klog(n)$\n",
    "\n",
    "Now we will compare the overall quality of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adj R2</th>\n      <th>BIC</th>\n      <th>AIC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.785773</td>\n      <td>-51.593272</td>\n      <td>-100.622007</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.774587</td>\n      <td>-44.719943</td>\n      <td>-86.205796</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     Adj R2        BIC         AIC\n0  0.785773 -51.593272 -100.622007\n1  0.774587 -44.719943  -86.205796"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "# create OLS models\n",
    "model1 = OLS(y1, X1).fit()\n",
    "model2 = OLS(y2, X2).fit()\n",
    "\n",
    "# model 1: get selection criteria information\n",
    "model1_r2adj = model1.rsquared_adj\n",
    "model1_BIC = model1.bic\n",
    "model1_AIC = model1.aic\n",
    "\n",
    "# model 2: get selection criteria information\n",
    "model2_r2adj = model2.rsquared_adj\n",
    "model2_BIC = model2.bic\n",
    "model2_AIC = model2.aic\n",
    "\n",
    "# create a dictionary of information and create dataframe\n",
    "dict1 = {'Adj R2': [model1_r2adj, model2_r2adj], 'BIC': [model1_BIC, model2_BIC], 'AIC': [model1_AIC, model2_AIC]}\n",
    "pd.DataFrame(dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first model has a higher $\\hat{R}^2$ and lower BIC and AIC\n",
    "\n",
    "THEREFORE, would choose model1 (which has linstsq and agesq) over model2\n",
    "\n",
    "We also have another criteria:\n",
    "\n",
    "Mallows' Cp:\n",
    "\n",
    "$C_p = n \\hat{\\sigma}^2 + 2K \\tilde{\\sigma}^2$\n",
    "\n",
    "where $\\tilde{\\sigma}^2$ is a preliminary estimator of $\\sigma^2$ (typically based on fitting a larger model i.e one with all the predicotrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>14.190144</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>14.757989</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "          Cp\n0  14.190144\n1  14.757989"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sigma squares from models\n",
    "m1_sig_sq = model1.mse_resid\n",
    "m2_sig_sq = model2.mse_resid\n",
    "\n",
    "# get degrees of freedom\n",
    "m1_k = model1.df_model\n",
    "m2_k = model2.df_model\n",
    "\n",
    "# calculate Mallows' Cp for each model\n",
    "model1_cp = model1.nobs * m1_sig_sq + 2 * m1_k * m1_sig_sq\n",
    "model2_cp = model2.nobs * m2_sig_sq + 2 * m2_k * m2_sig_sq\n",
    "\n",
    "# create dictionary and dataframe\n",
    "dict2 = {'Cp': [model1_cp, model2_cp]}\n",
    "pd.DataFrame(dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1 has a smaller Cp than model 2\n",
    "\n",
    "THEREFORE, model 1 is preferred using this criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shibata, final prediction error, generalized cross-validation\n",
    "\n",
    "shibata = $\\hat{\\sigma}^2 (1 + \\frac{2K}{n})$\n",
    "\n",
    "FPE = $\\hat{\\sigma}^2 (\\frac{1 + K/n}{1 - K/n})$\n",
    "\n",
    "GCV = $\\frac{n\\hat{\\sigma}^2}{(n-K)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Shibata</th>\n      <th>FPE1</th>\n      <th>GCV1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.044206</td>\n      <td>0.044325</td>\n      <td>0.000138</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.045975</td>\n      <td>0.046062</td>\n      <td>0.000144</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    Shibata      FPE1      GCV1\n0  0.044206  0.044325  0.000138\n1  0.045975  0.046062  0.000144"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually calculating Shibata, FPE, and GCV for model1\n",
    "shibata1 = m1_sig_sq * (1 + 2 * m1_k/model1.nobs)\n",
    "\n",
    "FPE1_fraction = (1 + m1_k / model1.nobs) / (1 - m1_k / model1.nobs)\n",
    "FPE1 = m1_sig_sq * FPE1_fraction\n",
    "\n",
    "GCV1 = (model1.nobs * m1_sig_sq) / (model1.nobs - m1_k)**2\n",
    "\n",
    "# manually calculating Shibata, FPE, and GCV for model2\n",
    "shibata2 = m2_sig_sq * (1 + 2 * m2_k/model2.nobs)\n",
    "\n",
    "FPE2_fraction = (1 + m2_k / model2.nobs) / (1 - m2_k / model2.nobs)\n",
    "FPE2 = m2_sig_sq * FPE2_fraction\n",
    "\n",
    "GCV2 = (model2.nobs * m2_sig_sq) / (model2.nobs - m2_k)**2\n",
    "\n",
    "# create dataframe with information\n",
    "dict3 = {'Shibata': [shibata1, shibata2], 'FPE1': [FPE1, FPE2], 'GCV1': [GCV1, GCV2]}\n",
    "pd.DataFrame(dict3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model1 has smaller shibata, FPE1, and GCV1, so therefore model1 is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation\n",
    "\n",
    "$CV = \\frac{1}{n} \\sum_{i=1}^n \\tilde{e_i}^2$\n",
    "\n",
    "where $\\tilde{e_i}$ are the least squares leave-one-out prediction errors\n",
    "\n",
    "$\\tilde{e_i} = (1-h_{ii})^{-1} \\hat{e_i}$\n",
    "\n",
    "We define out of sample mean squared error as \n",
    "\n",
    "$\\tilde{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n \\tilde{e_i}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.044224</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.045975</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         CV\n0  0.044224\n1  0.045975"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate cross validation criteria\n",
    "CV1 = (model1.resid/(1 - model1.get_influence().hat_matrix_diag))**2\n",
    "\n",
    "CV2 = (model2.resid/(1 - model2.get_influence().hat_matrix_diag))**2\n",
    "\n",
    "# display dataframe\n",
    "dict4 = {'CV': [CV1.mean(),CV2.mean()]}\n",
    "pd.DataFrame(dict4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, model1 has a smaller CV than model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST OF DOC:\n",
    "\n",
    "* Relationship among Selection Criteria\n",
    "\n",
    "* Consistent Selection\n",
    "\n",
    "* Information Criteria\n",
    "\n",
    "* Asymptotic Selection Optimality\n",
    "\n",
    "skipped - mostly explaining topic, not much code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}