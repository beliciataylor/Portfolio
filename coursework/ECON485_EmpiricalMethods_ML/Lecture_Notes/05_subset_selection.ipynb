{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection\n",
    "* approach involves identifying a subset of p features (predictors) out of k that we think are related to the response\n",
    "\n",
    "* then fit a model using OLS on the reduced set of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# import dataset \n",
    "hprice2 = pd.read_stata('http://fmwww.bc.edu/ec-p/data/wooldridge/hprice2.dta')\n",
    "\n",
    "# write specification\n",
    "f = 'lprice ~ lnox + lproptax + crime + rooms + dist + radial + stratio + lowstat'\n",
    "\n",
    "# create design matrices\n",
    "y1, X1 = patsy.dmatrices(f, data=hprice2, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing: demean outcome and features so all models can be fitted without an intercept\n",
    "y = y1.sub(y1.mean())\n",
    "X = X1.sub(X1.mean()).drop('Intercept',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Selection\n",
    "* fit separate OLS regression best subset for each possible combination of k predictors\n",
    "\n",
    "* fit all k models that contain exactly one predictor, then all ${k \\choose 2} = \\frac{k(k-1)}{2}$ models that contain exactly two predictors, etc.\n",
    "\n",
    "* look at resulting models to identify the best one\n",
    "\n",
    "* number of models to consider grows rapidly as k increases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Best Subset Selection\n",
    "\n",
    "1. Let $\\mathcal{M_0}$ denote null model (i.e no predictors) - predicts sample mean for each observation\n",
    "\n",
    "2. For p = 1,2,..., k:\n",
    "\n",
    "a) Fit all ${k \\choose p}$ models that contain exactly p predictors\n",
    "\n",
    "b) Pick best among these ${k \\choose p}$ models and call it $\\mathcal{M_p}$; in this case, best is defined as having the smallest RSS or, equivalently, the largest $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes the predictors selected and subsets the X design matrix\n",
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "\n",
    "    # calculate RSS\n",
    "    RSS = regr.ssr\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "# define a function that selects the best model with p number of predictors\n",
    "def getBest(p):\n",
    "    \n",
    "    # empty list to collect model and RSS \n",
    "    results = []\n",
    "    \n",
    "    # iterate through the different predictor combinations subject to a limit of p predictors\n",
    "    for combo in itertools.combinations(X.columns, p):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Create a dataframe of the results\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model\n",
    "\n",
    "# dataframe where best models will be collected\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "# for loop that collects the best model for each number of predictors\n",
    "for i in range(1,9):\n",
    "    models_best.loc[i] = getBest(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make graphs of results - will do later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Selection: Forward Stepwise Selection\n",
    "* computational reasoning, previous 'best subset selection' can't be applied with very large k's\n",
    "\n",
    "* this algorithm begins with a model containing no predictors, then adds predictors to the model one-at-a-time until all predictors are in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Forward Stepwise Selection\n",
    "\n",
    "1. Let $\\mathcal{M_0}$ denote the null model which contains no predictors\n",
    "\n",
    "2. For p = 0,1,2,..., k-1\n",
    "\n",
    "A. Consider all k-p models that augment the predictors in $\\mathcal{M_p}$ with one additional predictor\n",
    "\n",
    "B. Choose the best among these k-p models; call it $\\mathcal{M_{p+1}}$; best is defined as having smallest RSS or highest $R^2$\n",
    "\n",
    "3. Then select a single best model form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    # create an empty list\n",
    "    results = []\n",
    "    \n",
    "    # take each p in the remaining predictors and create a model & take RSS\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # collect models in a data frame\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model\n",
    "\n",
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Selection: Backward Stepwise Selection\n",
    "* begins with full least squares model containing all k regressors\n",
    "\n",
    "* then iteractively remove the least useful regressors one-at-a-time until there are no regressors in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Backward Stepwise Selection\n",
    "\n",
    "1. Let $\\mathcal{M_k}$ deonte the full model that contains all k predictors\n",
    "\n",
    "2. For p = k, k-1, ..., 1\n",
    "\n",
    "A. Consider all p models that contain all but one of the predictors in $\\mathcal{p}$ for a total of p-1 predictors\n",
    "\n",
    "B. Choose the best model among these p models; call it $\\mathcal{M_{p-1}}$; best defined as having smallest RSS or highest $R^2$\n",
    "\n",
    "3. Select single best model from among $\\mathcal{M_0},..., \\mathcal{M_k}$ using a model selection criteria previously discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied - need to look over code\n",
    "\n",
    "def backward(predictors):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model\n",
    "\n",
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
    "\n",
    "predictors = X.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Optimal Model\n",
    "* previous algorithms: good at selecting a model that best fit the training data set (since they minimize the RSS which is equivalent to maximizing R^2 for these observations)\n",
    "\n",
    "* does not necessarily mean they minimize the RSS in any validation set\n",
    "\n",
    "* therefore, the training set RSS and the training set R2 cannot be used to select from among a set of models with different numbers of variables if we're using this model for prediction\n",
    "\n",
    "* can replace step 3 in these algorithms with a step where the cross-validated prediction error is calculated using observations in the validation set\n",
    "\n",
    "* can use other model selection criteria ($\\bar{R}^2$, Cp, AIC, BIC, etc) but if you want to know about the prediction power of model, cross validation prediction error is more natural choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied - look over later\n",
    "\n",
    "def processSubset(feature_set, X_train, y_train, X_test, y_test):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y_train,X_train[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X_test[list(feature_set)]) - y_test.iloc[:,0])**2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def forward(predictors, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X_train.columns if p not in predictors]\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p], X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the lowest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "        \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model\n",
    "\n",
    "# number of folds\n",
    "k = 10\n",
    "np.random.seed(seed=1)\n",
    "folds = np.random.choice(k, size = len(y), replace = True)\n",
    "\n",
    "# Create a DataFrame to store the results of our upcoming calculations\n",
    "cv_errors = pd.DataFrame(columns=range(1,k+1), index=range(1,9))\n",
    "\n",
    "models_cv = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "# Outer loop iterates over all folds\n",
    "for j in range(1,k+1):\n",
    "\n",
    "    # Reset predictors\n",
    "    predictors = []\n",
    "    \n",
    "    # Inner loop iterates over each size i\n",
    "    for i in range(1,len(X.columns)+1):    \n",
    "    \n",
    "        # The perform forward selection on the full dataset minus the jth fold, test on jth fold\n",
    "        models_cv.loc[i] = forward(predictors,  X[folds != (j-1)], y[folds != (j-1)], X[folds == (j-1)], y[folds == (j-1)\n",
    "        \n",
    "        # Save the cross-validated error for this fold\n",
    "        cv_errors[j][i] = models_cv.loc[i][\"RSS\"]\n",
    "\n",
    "        # Extract the predictors\n",
    "        predictors = models_cv.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "cv_errors\n",
    "\n",
    "cv_mean = cv_errors.apply(np.mean, axis=1)\n",
    "\n",
    "#extract the best model and estimate its parameters using the entire data set\n",
    "\n",
    "print(sm.OLS(y1,sm.add_constant(X1[models_cv.loc[7, \"model\"].model.exog_names])).fit().summary())\n",
    "print(sm.OLS(y,X[models_cv.loc[7, \"model\"].model.exog_names]).fit().summary())\n"
   ]
  }
 ]
}